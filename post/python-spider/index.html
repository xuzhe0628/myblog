    <!DOCTYPE html>
<html lang="zh-cn">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Xu Zhe&#39;s Blog">
		<meta name="description" content="A blog about data science, coding and dog.">
		<meta name="generator" content="Hugo 0.20.6" />
		<title>Python Crawler and Scraper Fundamentals &middot; Xu Zhe&#39;s Blog</title>
		<link rel="shortcut icon" href="http://xuzhe91.com/images/favicon.ico">
		<link rel="stylesheet" href="http://xuzhe91.com/css/style.css">
		<link rel="stylesheet" href="http://xuzhe91.com/css/highlight.css">
		

		
		<link rel="stylesheet" href="http://xuzhe91.com/css/font-awesome.min.css">
		

		
	</head>

    <body>
       <nav class="main-nav">
	
	
		<a href='http://xuzhe91.com/'> <span class="arrow">←</span>Home</a>
	
	<a href='http://xuzhe91.com/post'>Archive</a>
	<a href='http://xuzhe91.com/tags'>Tags</a>
	<a href='http://xuzhe91.com/about'>About</a>

	

	
</nav>

        <section id="wrapper">
            <article class="post">
                <header>
                    <h1>
                        Python Crawler and Scraper Fundamentals
                    </h1>
                    <h2 class="headline">
                    Jun 19, 2017 00:40
                    · 1683 words
                    · 8 minutes read
                      <span class="tags">
                      
                      
                          
                              <a href="http://xuzhe91.com/tags/python">python</a>
                          
                              <a href="http://xuzhe91.com/tags/scraper">scraper</a>
                          
                              <a href="http://xuzhe91.com/tags/crawler">crawler</a>
                          
                              <a href="http://xuzhe91.com/tags/english">english</a>
                          
                              <a href="http://xuzhe91.com/tags/spider">spider</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    

<h1 id="python-web-crawler-and-scraper-fundamentals">Python Web Crawler and Scraper Fundamentals</h1>

<p><strong>Github</strong>: <a href="https://github.com/xuzhe0628/python-spider-scraper-fundamentals">https://github.com/xuzhe0628/python-spider-scraper-fundamentals</a></p>

<ul>
<li>## Content

<ol>
<li>Web crawler and scraper</li>
<li>HTTP Request: GET and POST</li>
<li>Capture and parse web via urllib</li>
<li>Powerful requests: HTTP for Humans</li>
<li>Web scraper framework: scrapy</li>
<li>Using API</li>
</ol></li>
</ul>

<p>## Prerequisite ##</p>

<ul>
<li>Python 3.5 or later version</li>
<li>MongoDB (needed if you want to run the Scrapy example)</li>
</ul>

<p>### 1. Web crawler and scraper
  <a href="http://stackoverflow.com/questions/3207418/crawler-vs-scraper">http://stackoverflow.com/questions/3207418/crawler-vs-scraper</a></p>

<p>A crawler get web pages from a starting page based one some rules or conditions. It downloads whatever it found.</p>

<p>A scraper extracts data from pages that we downloaded by a crawler, so that we can store them into databases or other data management systems.</p>

<p>### 2. HTTP Request: GET and POST
  <a href="https://www.w3schools.com/tags/ref_httpmethods.asp">https://www.w3schools.com/tags/ref_httpmethods.asp</a></p>

<p>The Hypertext Transfer Protocol (HTTP) is designed to enable communications between clients and servers. HTTP works as a request-response protocol between a client and server. A web browser may be the client, and an application on a computer that hosts a web site may be the server.</p>

<p>Two commonly used methods for a request-response between a client and server are: GET and POST.</p>

<ul>
<li>GET - Requests data from a specified resource</li>
<li>POST - Submits data to be processed to a specified resource</li>
</ul>

<p>#### 2.1 The GET Method
  Note that the query string (name/value pairs) is sent in the URL of a GET request:</p>

<p><code>/test/demo_form.php?name1=value1&amp;name2=value2</code></p>

<p>Some other notes on GET requests:
  * GET requests can be cached.
  * GET requests remain in the browser history.
  * GET requests can be bookmarked.
  * GET requests should never be used when dealing with sensitive data.
  * GET requests have length restrictions. The limit is dependent on both the server and the client used (and if applicable, also the proxy the server or the client is using).
  * GET requests should be used only to retrieve data.</p>

<p><strong>Example</strong></p>

<p>![]<a href="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/GET.png)">https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/GET.png)</a></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/GETRESULT.PNG" alt="" /></p>

<p>#### 2.2 The POST Method
  Note that the query string (name/value pairs) is sent in the HTTP message body of a POST request:</p>

<pre><code>  POST /test/demo_form.php HTTP/1.1
  Host: w3schools.com
  name1=value1&amp;name2=value2
</code></pre>

<p>Some other notes on POST requests:
  * POST requests are never cached.
  * POST requests do not remain in the browser history.
  * POST requests cannot be bookmarked.
  * POST requests have no restrictions on data length.</p>

<p><strong>Example</strong></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/LOGIN.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/FORM.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/POST.png" alt="" /></p>

<p>### 3. Capture and parse web page using urllib</p>

<p>Python has a standard package urllib for handling URLs. We can use it to write basic web crawler and scraper.</p>

<ul>
<li><a href="https://docs.python.org/3.5/library/urllib.request.html#module-urllib.request"><code>urllib.request</code></a> for opening and reading URLs</li>
<li><a href="https://docs.python.org/3.5/library/urllib.error.html#module-urllib.error"><code>urllib.error</code></a> containing the exceptions raised by <a href="https://docs.python.org/3.5/library/urllib.request.html#module-urllib.request"><code>urllib.request</code></a></li>
<li><a href="https://docs.python.org/3.5/library/urllib.parse.html#module-urllib.parse"><code>urllib.parse</code></a> for parsing URLs</li>
<li><a href="https://docs.python.org/3.5/library/urllib.robotparser.html#module-urllib.robotparser"><code>urllib.robotparser</code></a> for parsing <code>robots.txt</code> files</li>
</ul>

<pre><code class="language-python">  import urllib.request
  import re

  # Regex to match the repo name
  REPO_REGEX = re.compile(b'.*/(.*/python).*', re.IGNORECASE, )

  pythonRepo = urllib.request.urlopen(
      'https://github.com/search?utf8=%E2%9C%93&amp;q=python&amp;type=')
  # The url we provided is for sending the query keyword to the web server via GET method.

  # Save the response as a html file
  # &quot;wb&quot; means the mode opening the file. &quot;w&quot; means open the file in write mode.
  # &quot;b&quot; means open the file in binary mode.

  with open('ResultPage.html', 'wb') as resultPage:
      resultPage.write(pythonRepo.read())

  # Request the query again and find and print the repo name.
  pythonRepo = urllib.request.urlopen(
      'https://github.com/search?utf8=%E2%9C%93&amp;q=python&amp;type=')

  for line in pythonRepo:
      if REPO_REGEX.match(line):
          print(REPO_REGEX.match(line).group(1))
</code></pre>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/URLLIBRESULT.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/URLLIBWEB.PNG" alt="" /></p>

<p>### 4. Powerful Requests: HTTP for Humans</p>

<p><a href="http://docs.python-requests.org/en/master/">Requests</a> is an elegant and simple HTTP library for Python, built for human beings. It is a powerful, safe and user-friendly library for handling HTTP requests and responses. Let&rsquo;s use it to submitting a post request and parse the response.</p>

<p>As it is not a standard library, install it via pip.</p>

<p><code>pip install requests</code></p>

<pre><code class="language-python">  import requests
  from bs4 import BeautifulSoup # standard library for parsing html page

  # header for simulating the browser
  headers = {
      &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0&quot;,
      &quot;Accept&quot;: &quot;application/json, text/javascript, */*; q=0.01&quot;,
      &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded; charset=UTF-8&quot;,
      &quot;X-Requested-With&quot;: &quot;XMLHttpRequest&quot;,
  }

  # creare a session to hold all browser information
  gitSession = requests.Session()

  # get the auth token from github login page
  soup = BeautifulSoup(gitSession.get('https://github.com/login', 
      headers=headers).text,
      'html.parser')
  # find the element containing the auth_token by selector
  auth_token = soup.find(&quot;input&quot;, {&quot;name&quot;: &quot;authenticity_token&quot;}).attrs['value']
  print('auth_token: {}'.format(auth_token))

  # prepare the login data using the auth token we got
  payload = {
      &quot;commit&quot;: &quot;Sign in&quot;,
      &quot;authenticity_token&quot;: auth_token,
      &quot;login&quot;: &quot;your login&quot;, # fill your username
      &quot;password&quot;: &quot;your password&quot; # fill your password
  }

  # send a post request to session page to login
  # we will need the session as Github request enable cookie and the session can
  # help us to keep the cookie for different requests
  login = gitSession.post('https://github.com/session', 
      data=payload)

  # find my repo names by css selector
  soup = BeautifulSoup(login.text, 'html.parser')
  repos = soup.find_all(&quot;span&quot;, class_=&quot;repo&quot;)
  for repo in repos:
      print(repo.text)
</code></pre>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/LOGINRESULT.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/GITREPO.PNG" alt="" /></p>

<p>### 5. Web scraper framework: Scrapy</p>

<p><a href="https://docs.scrapy.org/en/latest/intro/overview.html">Scrapy</a> is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.</p>

<p>You can use it to build distributed crawler system, use pipeline to process data captured and provide functions to handle exceptions and websites with anti-crawling mechanism.</p>

<p>Let&rsquo;s write a spider to crawl movie information from <code>THE MOVIE DB</code> and store it to MongoDB using item and pipeline.</p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/MOVIE.PNG" alt="" /></p>

<p>As it is not a standard library, install it using pip.</p>

<p><code>pip install scrapy</code></p>

<p>You will also need <code>pymongo</code> to access your MongoDB.</p>

<p><code>pip install pymongo</code></p>

<p>Use below command to create a scrapy project.</p>

<p><code>scrapy startproject tutorial</code></p>

<p>This will create a <code>tutorial</code> directory with the following contents:</p>

<pre><code>  tutorial/
      scrapy.cfg            # deploy configuration file

      tutorial/             # project's Python module, you'll import your code from here
          __init__.py

          items.py          # project items definition file

          pipelines.py      # project pipelines file

          settings.py       # project settings file

          spiders/          # a directory where you'll later put your spiders
              __init__.py
</code></pre>

<p>Create a python file called <code>Movie.py</code> under <code>tutorial/spiders</code> folder.</p>

<p><strong>Item</strong></p>

<p>Item can help you to modeling the data you extracted from the web pages. It is very simple to define an item. As we are crawling movie data, we create a movie item by adding below codes to <code>items.py</code> file. It is very simple. You only need to define the field in a class inheriting <code>scrapy.Item</code> class. The fields are instance of <code>scrapy.Field</code> class.</p>

<pre><code class="language-python">  import scrapy

  class MovieItem(scrapy.Item):
      _id = scrapy.Field()
      movie = scrapy.Field()
      rating = scrapy.Field()
      release_date = scrapy.Field()
      genres = scrapy.Field()
      overview = scrapy.Field()
</code></pre>

<p><strong>Spider</strong></p>

<p>Spider is the core script for finding URL for next pages and scraping data. As we defined an item, we will populate the item using the data we got. Create the spider named <code>Movie.py</code> in <code>spiders</code> folder.</p>

<pre><code class="language-python">  import scrapy
  from ..items import MovieItem # import the item we defined

  class MovieSpider(scrapy.Spider):
      # this is the name of the spider,
      # it will be used when we want to start the cralwer
      name = &quot;Movie&quot; 
      start_urls = [
          'https://www.themoviedb.org/movie',
      ]

      def parse(self, response):
          # use css selector to locate the elements containing movie information
          # populate the item with data we got
          for movie in response.css('div.info'):
              mItem = MovieItem()
              mItem['_id'] = ('https://www.themoviedb.org' + 
                  movie.css('p.flex a::attr(&quot;href&quot;)').extract_first())
              mItem['movie'] = movie.css('p.flex a::text').extract_first()
              mItem['rating'] = movie.css('span.vote_average::text').extract_first()
              mItem['release_date'] = movie.css('span.release_date::text').extract_first()
              mItem['genres'] = movie.css ('span.genres::text').extract_first()
              mItem['overview'] = movie.css ('p.overview::text').extract_first()
              yield mItem
          
          # return the url of next page
          next_page = response.css('p.right.pagination a::attr(&quot;href&quot;)').extract()
          if len(next_page) &gt; 1:
              next_page = next_page[1]
          else:
              next_page = next_page[0]
          if next_page is not None:
              yield response.follow(next_page, self.parse)
</code></pre>

<p><strong>Pipeline</strong></p>

<p>Pipeline is a powerful module. We can use it to do data cleansing, dedup, formating and calculating before saving the data. We create a item pipeline that directly save the item into MongoDB. Add below codes to <code>pipelines.py</code> file.</p>

<pre><code class="language-python">  import pymongo

  class MoviePipeline(object):
      # action that will be taken when the crawler started
      def open_spider(self, spider):
          # connect to local mongodb on port 27018, default port is 27017
          self.client = pymongo.MongoClient(&quot;mongodb://localhost:27018&quot;)
          # connect to database test
          self.test = self.client['test']
          # connect to collection Movie
          self.movie = self.test['Movie']
      
      def process_item(self, item, spider):
          # insert the item to mongodb
          self.movie.insert_one(item)
          return item

      def close_spider(self, spider):
          pass
</code></pre>

<p>Enable the item pipeline by adding below codes to <code>settings.py</code>.</p>

<pre><code class="language-python">  ITEM_PIPELINES = {
      'tutorial.pipelines.MoviePipeline': 300,
  }
</code></pre>

<p>Start the crawler.</p>

<p><code>scrapy crawl Movie</code></p>

<p><strong>Finally!!</strong></p>

<p>It took me less than 20 minutes to crawled 19633 movies, not bad.</p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/MOVIECRAWL.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/MOVIECOUNT.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/MOVIEDETAIL.PNG" alt="" /></p>

<p>### 6. Using API ###</p>

<p>Many services, such as Google Map, OneDrive and Box, has provided official API for user to automatically some routine tasks. With API, we don&rsquo;t need to build the post or get request by ourself.</p>

<p>#### OneDrive API ####</p>

<p>[onedrivesdk][<a href="https://github.com/OneDrive/onedrive-sdk-python">https://github.com/OneDrive/onedrive-sdk-python</a>] is the python API for OneDrive. OneDrive API uses the standard OAuth 2.0 <em>authentication</em> scheme to authenticate users and generate access tokens. To use the API to login into your OneDrive account, you need to understand the <a href="https://dev.onedrive.com/auth/msa_oauth.htm">authentication mechanism</a>.</p>

<p>Below script is using Code Flow for authentication and print out the basic account information after login.</p>

<pre><code class="language-python">  import onedrivesdk

  redirect_uri = 'https://localhost:8080'
  client_id = '' # fill in your client id
  client_secret = '' # fill in your client id
  api_base_url = 'https://api.onedrive.com/v1.0/'
  scopes = ['wl.signin', 'wl.offline_access', 'onedrive.readwrite']

  def auth_onedrive(client_id, scopes, api_base_url, redirect_uri):
      http_provider = onedrivesdk.HttpProvider()
      auth_provider = onedrivesdk.AuthProvider(
          http_provider=http_provider,
          client_id=client_id,
          scopes=scopes)

      client = onedrivesdk.OneDriveClient(api_base_url, auth_provider,
          http_provider)
      auth_url = client.auth_provider.get_auth_url(redirect_uri)
      # Ask for the code
      print('Paste this URL into your browser, approve the app\'s access.')
      print(
          'Copy everything in the address bar after &quot;code=&quot;, and'
          ' paste it below.')
      print(auth_url)
      with open('authurl.txt', 'w') as authurl:
          authurl.write(auth_url)
      code = input('Paste code here: ')

      client.auth_provider.authenticate(code, redirect_uri, client_secret)
      
      return client
    
  def upload_onedrive(client, upload_path, upload_file):
      returned_item = client.item(drive='me', id='root')
      returned_item = returned_item.children[upload_file]
      upload_item = returned_item.upload(os.path.join(upload_path, upload_file))
      
  client = auth_onedrive(client_id, scopes, api_base_url, redirect_uri)
  #get the top three elements of root, leaving the next page for more elements
  collection = client.item(drive='me', id='root').children.request(top=3).get()

  for item in collection:
      print(item.__dict__)

  # upload_onedrive(client, base_folder, archive_folder + '.zip')
</code></pre>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/ONEDRIVEAUTH.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/xuzhe0628/python-spider-scraper-fundamentals/master/Images/ONEDRIVEAPI.PNG" alt="" /></p>

<p>### Summary ###</p>

<p>This sharing only covers basic and some intermediate topics for web crawler and scraper. It is not hard to write a &ldquo;toy&rdquo; crawler, but many modern websites have very complex anti-crawling mechanisms that makes its&rsquo;s very challenging to crawl data from them. Moreover, some crawling tasks are challenging because of the data volume, data quality or efficiency.</p>

<p>To take a further step in this topic, I will recommend you to further look at below topics.</p>

<ul>
<li>HTTP Protocol</li>
<li>TCP/IP Protocl</li>
<li>Socket Programming</li>
<li>JavaScript/AJAX</li>
<li>Distributed Spider System</li>
<li>Multi-thread Programming</li>
<li>Regex/css selector/xPath</li>
<li>Okta/OAuth</li>
<li>XML/JSON/SOAP</li>
<li>HTMLUnit，PhantomJS，SlimerJS ，CasperJS</li>
<li>Identifying code/OCR</li>
</ul>

                </section>
            </article>

            

            
                <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'xuzhe91'; 

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

            

            

            <footer id="footer">
    
        <div id="social">

	
	
    <a class="symbol" href="https://github.com/xuzhe0628">
        <i class="fa fa-github-square"></i>
    </a>
    


</div>

    
    <p class="small">
    
       © Copyright 2017 <i class="fa fa-heart" aria-hidden="true"></i> Xu Zhe&#39;s Blog
    
    </p>
    <p class="small">
        Powered by <a href="http://www.gohugo.io/">Hugo</a> Theme By <a href="https://github.com/nodejh/hugo-theme-cactus-plus">nodejh</a>
    </p>
</footer>
        </section>

        <script src="http://xuzhe91.com/js/jquery-2.2.4.min.js"></script>
<script src="http://xuzhe91.com/js/main.js"></script>
<script src="http://xuzhe91.com/js/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>






    </body>
</html>
