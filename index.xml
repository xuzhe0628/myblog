<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xu Zhe&#39;s Blog</title>
    <link>http://xuzhe91.com/</link>
    <description>Recent content on Xu Zhe&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 28 May 2017 20:30:40 +0800</lastBuildDate>
    
	<atom:link href="http://xuzhe91.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Curse of Dimensionality</title>
      <link>http://xuzhe91.com/post/machine-learning-curse-of-dimensionality/</link>
      <pubDate>Sun, 28 May 2017 20:30:40 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/machine-learning-curse-of-dimensionality/</guid>
      <description>Curse of Dimensionality As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.</description>
    </item>
    
    <item>
      <title>Causes of Error in Machine Learning</title>
      <link>http://xuzhe91.com/post/machine-learning-errors/</link>
      <pubDate>Sun, 28 May 2017 20:04:58 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/machine-learning-errors/</guid>
      <description>Bias Bias due to a model being unable to represent the complexity of the underlying data. A high Bias model is said to underfit the data.
Bias occurs when a model has enough data but is not complex enough to capture the underlying relationships. As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction. This is known as underfitting. Simply put, bias occurs when we have an inadequate model.</description>
    </item>
    
    <item>
      <title>MLND-Machine Learning Algorithm Performance Evaluation Metrics</title>
      <link>http://xuzhe91.com/post/mlnd-evaluation-metrics/</link>
      <pubDate>Fri, 26 May 2017 00:00:05 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/mlnd-evaluation-metrics/</guid>
      <description>Type Name Definition Comments     Classification Accuracy number of correctly identified instances / all instances Not ideal for skewed data. Not ideal for situations that you care one classification more than the rest. For example, if you create a model to predict cancer, you may care more about whether the people have cancer other than he does not.   Classification Precision True Positive / (True Positive + False Positive) Out of all the items labeled as positive, how many truly belong to the positive class.</description>
    </item>
    
    <item>
      <title>Python spider example--Building Post Request</title>
      <link>http://xuzhe91.com/post/python-spider/</link>
      <pubDate>Thu, 25 May 2017 21:22:23 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/python-spider/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Scikit-learn Cheat Sheet</title>
      <link>http://xuzhe91.com/post/scikit-learn-cheatsheet/</link>
      <pubDate>Thu, 25 May 2017 21:21:14 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/scikit-learn-cheatsheet/</guid>
      <description>Some variables used in the cheat sheet.
clf # the model that we trained X_test # the test dataset y_test # the target of the test dataset  Preprocessing Dataset Split training dataset test_size decides the proportion of instances split from the training set as test data. random_state decides whether the data is randomly chosen.
from sklearn import cross_validation X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)  Evaluation Metrics Confusion Matrix from sklearn.</description>
    </item>
    
    <item>
      <title>MongoDB Cheat Sheet</title>
      <link>http://xuzhe91.com/post/mongodb-cheatsheet/</link>
      <pubDate>Thu, 25 May 2017 21:20:51 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/mongodb-cheatsheet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pandas Cheat Sheet</title>
      <link>http://xuzhe91.com/post/pandas-cheatsheet/</link>
      <pubDate>Thu, 25 May 2017 21:20:43 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/pandas-cheatsheet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Central Tendency and Variability of Data</title>
      <link>http://xuzhe91.com/post/statistic-fundamentals/</link>
      <pubDate>Thu, 25 May 2017 21:17:09 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/statistic-fundamentals/</guid>
      <description>Statistics provide several simple but powerful concepts that we can use to measure the central tendency and variability of datasets. I just want to write my understanding about these concepts.
Measures of Central Tendency Mode
Median
Mean
Variability Quartile
Standard Deviation</description>
    </item>
    
    <item>
      <title>台湾行计划</title>
      <link>http://xuzhe91.com/post/taipei/</link>
      <pubDate>Tue, 23 May 2017 21:28:19 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/taipei/</guid>
      <description>台北旅游计划 简单计划一下六月份的台北之旅。 6月2号 入住酒店 午饭后可游览西门町、信义商圈 晚上游览台北101大厦 6月3号 游览九份（据说夜景很卖，</description>
    </item>
    
    <item>
      <title>2017学习计划</title>
      <link>http://xuzhe91.com/post/2017-learn/</link>
      <pubDate>Wed, 17 May 2017 16:53:50 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/2017-learn/</guid>
      <description>今年准备提升自己在计算机科学，统计学，机器学习和数据挖掘方面的知识，达到能够在工作中应用常见机器学习和数据挖掘算法的能力。所以今年我参加了一</description>
    </item>
    
    <item>
      <title>Python学习资料推荐</title>
      <link>http://xuzhe91.com/post/python-books/</link>
      <pubDate>Wed, 17 May 2017 16:12:17 +0800</pubDate>
      
      <guid>http://xuzhe91.com/post/python-books/</guid>
      <description>总结一下我用过的Python书籍或者学习资料。排名不分先后。 廖雪峰的Python教程 很好的入门教程，讲解深入浅出。还有一个实际的项目可供练手</description>
    </item>
    
  </channel>
</rss>